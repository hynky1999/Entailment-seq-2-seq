@misc{bowmanLargeAnnotatedCorpus2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  date = {2015-08-21},
  number = {arXiv:1508.05326},
  eprint = {1508.05326},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1508.05326},
  url = {http://arxiv.org/abs/1508.05326},
  urldate = {2022-10-27},
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/kydliceh/Zotero/storage/YV65JKUB/Bowman et al. - 2015 - A large annotated corpus for learning natural lang.pdf;/home/kydliceh/Zotero/storage/GGRQVZJP/1508.html}
}

@online{CS230Recurrent,
  title = {{{CS}} 230 - {{Recurrent Neural Networks Cheatsheet}}},
  url = {https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#},
  urldate = {2022-10-30},
  file = {/home/kydliceh/Zotero/storage/MC5KGWD4/cheatsheet-recurrent-neural-networks.html}
}

@misc{gravesSpeechRecognitionDeep2013,
  title = {Speech {{Recognition}} with {{Deep Recurrent Neural Networks}}},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  date = {2013-03-22},
  number = {arXiv:1303.5778},
  eprint = {1303.5778},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1303.5778},
  urldate = {2022-10-31},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \textbackslash emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/home/kydliceh/Zotero/storage/5F963F6U/Graves et al. - 2013 - Speech Recognition with Deep Recurrent Neural Netw.pdf;/home/kydliceh/Zotero/storage/KTYRQEFH/1303.html}
}

@inproceedings{maccartneyModelingSemanticContainment2008,
  title = {Modeling {{Semantic Containment}} and {{Exclusion}} in {{Natural Language Inference}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Computational Linguistics}} ({{Coling}} 2008)},
  author = {MacCartney, Bill and Manning, Christopher D.},
  date = {2008-08},
  pages = {521--528},
  publisher = {{Coling 2008 Organizing Committee}},
  location = {{Manchester, UK}},
  url = {https://aclanthology.org/C08-1066},
  urldate = {2022-10-27},
  eventtitle = {{{COLING}} 2008},
  file = {/home/kydliceh/Zotero/storage/6DEQ7LMJ/MacCartney and Manning - 2008 - Modeling Semantic Containment and Exclusion in Nat.pdf}
}

@inproceedings{penningtonGloVeGlobalVectors2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014-10},
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  location = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  url = {https://aclanthology.org/D14-1162},
  urldate = {2022-10-31},
  eventtitle = {{{EMNLP}} 2014},
  file = {/home/kydliceh/Zotero/storage/4FZE9JGL/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@inproceedings{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  booktitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Smith, Leslie N.},
  date = {2017-03},
  pages = {464--472},
  publisher = {{IEEE}},
  location = {{Santa Rosa, CA, USA}},
  doi = {10.1109/WACV.2017.58},
  url = {http://ieeexplore.ieee.org/document/7926641/},
  urldate = {2022-11-13},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  eventtitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-5090-4822-9},
  langid = {english},
  file = {/home/kydliceh/Zotero/storage/9L6I7VXA/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf}
}

@online{StanfordNaturalLanguage,
  title = {The {{Stanford Natural Language Processing Group}}},
  url = {https://nlp.stanford.edu/projects/snli/},
  urldate = {2022-10-27},
  file = {/home/kydliceh/Zotero/storage/WYAMNX4Y/snli.html}
}

@misc{staudemeyerUnderstandingLSTMTutorial2019,
  title = {Understanding {{LSTM}} -- a Tutorial into {{Long Short-Term Memory Recurrent Neural Networks}}},
  author = {Staudemeyer, Ralf C. and Morris, Eric Rothstein},
  date = {2019-09-12},
  number = {arXiv:1909.09586},
  eprint = {1909.09586},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1909.09586},
  urldate = {2022-10-30},
  abstract = {Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are one of the most powerful dynamic classifiers publicly known. The network itself and the related learning algorithms are reasonably well documented to get an idea how it works. This paper will shed more light into understanding how LSTM-RNNs evolved and why they work impressively well, focusing on the early, ground-breaking publications. We significantly improved documentation and fixed a number of errors and inconsistencies that accumulated in previous publications. To support understanding we as well revised and unified the notation used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/kydliceh/Zotero/storage/2GLSCDSN/Staudemeyer and Morris - 2019 - Understanding LSTM -- a tutorial into Long Short-T.pdf;/home/kydliceh/Zotero/storage/PSXKD6ZQ/1909.html}
}

@misc{vaswaniAttentionAllYou2017b,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-10-30},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kydliceh/Zotero/storage/8JK2YP2Z/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/kydliceh/Zotero/storage/YFIJFG76/1706.html}
}

